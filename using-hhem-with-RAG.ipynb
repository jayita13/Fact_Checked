{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85034fee-f2f9-485b-8d06-de51234a2bbe",
   "metadata": {},
   "source": [
    "# HHEM with RAG\n",
    "\n",
    "This notebook explains what HHEM does and how it may be integrated into generative AI applications, and specifically into RAG (retrieval augmented generation).\n",
    "\n",
    "## HHEM overview\n",
    "\n",
    "So first, what is HHEM?\n",
    "\n",
    "In spite of the amazing power of LLM, they still do hallucinate. In some cases, where creativity is required, hallucinations are okay or even necessary, but in most enterprise use-cases a trusted response is needed.\n",
    "\n",
    "HHEM (Hughes Hallucination Evaluation Model) is a model that was built specifically to help LLM practitioners measure hallucinations. It is available for use on [Huggingface Hub](https://huggingface.co/vectara/hallucination_evaluation_model) and a public [leaderboard](https://huggingface.co/spaces/vectara/leaderboard) shows the likelihood of various LLMs (both commercial and open source) to hallucinate.\n",
    "\n",
    "At a basic level, HHEM is a classification neural network model that given two text strings (sentence A and sentence B) returns a score between 0...1 reflecting the extent to which sentence B is factually consistent with sentence A.\n",
    "\n",
    "Let's demonstrate this via an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0193c1db-6f43-4c92-87d0-ffe4005cc176",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ofer/miniconda3/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.9922, 0.0005, 0.0053, 0.2113, 0.9641, 0.1181], dtype=float32)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True, precision=4)\n",
    "\n",
    "model = CrossEncoder('vectara/hallucination_evaluation_model')\n",
    "scores = model.predict([\n",
    "    [\"A man walks his dog in the backyard.\", \"A guy walks his labradoodle in his backyard.\"],\n",
    "    [\"A person on a horse jumps over a broken down airplane.\", \"A person is at a diner, ordering an omelette.\"],\n",
    "    [\"A phone is ringing on the table\", \"A telephone is buzzing on the table\"],\n",
    "])\n",
    "\n",
    "example_pairs = [\n",
    "    # Good summary\n",
    "    {\"article\": \"The woman is playing mario cart while resting on the couch\",\n",
    "     \"summary\": \"The woman is playing a game resting\"},\n",
    "\n",
    "    # Bad summary\n",
    "    {\"article\": \"A person on a horse jumps over a broken down airplane.\", \n",
    "     \"summary\": \"A person is at a diner, ordering an omelette.\"},\n",
    "    \n",
    "    # Discourse link error\n",
    "    {\"article\": \"Goldfish are being caught weighing up to 2kg and koi carp up to 8kg and one metre in length\",\n",
    "     \"summary\": \"Koi carp can be as heavy as 2kg and as long as one meter\"}, \n",
    "\n",
    "    # Coreference error\n",
    "    {\"article\": \"Mr Katter said the Government believes Mr Gordon would quit after he was recently accused of domestic violence.\",\n",
    "     \"summary\": \"Mr Katter said he would quit after he was accused of domestic violence.\"},\n",
    "\n",
    "    {\"article\":\"The first vaccine for Ebola was approved by the FDA in 2019 in the US,     five years after the initial outbreak in 2014. To produce the vaccine,  scientists had to sequence the DNA of Ebola, then identify possible vaccines, and finally show successful clinical trials.  Scientists say a vaccine for COVID-19 is unlikely to be ready this year, although clinical trials have already started. \",\n",
    "     \"summary\":\"You won't get the COVID-19 vaccine this year.\"},\n",
    "\n",
    "    # extrinsic error\n",
    "    {\"article\": \"The plants were found during the search of a warehouse near Ashbourne on Saturday morning. Police said they were in 'an elaborate grow house'. A man in his late 40s was arrested at the scene.\",\n",
    "     \"summary\":\"Police have arrested a man in his late 40s after cannabis plants worth an estimated Â£100,000 were found in a warehouse near Ashbourne.\"}\n",
    "]\n",
    "\n",
    "scores = model.predict(\n",
    "    [ [p[\"article\"], p[\"summary\"]] for p in example_pairs ]\n",
    ")\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5ac733-22a2-4255-863a-3fd2f32ee606",
   "metadata": {},
   "source": [
    "We can see HHEM in action:\n",
    "\n",
    "- The first example is a very good summary, and correspondingly the score is very close to 1.0.\n",
    "- The second case, the summary is completely different than the original text, and the score is very low.\n",
    "- Example 3 shows an error where the summary associates 2kg weight to Koi where it should be 8kg. Thus the low score\n",
    "- Examples 4, 5 and 6 show additional nuanaced examples\n",
    "\n",
    "Overall, you can quickly get the sense of how summarization can go wrong and how HHEM does a great job scoring the summary based on the article."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9892ad2-e22b-4545-abb6-df735a1015ee",
   "metadata": {},
   "source": [
    "## Using HHEM in RAG\n",
    "\n",
    "RAG, or retrieval augmented generation, adapts LLMs to answer user questions based on facts retrieved from custom data. It is an extremely practical architecture that reduces hallucinations, increases trust, and provides an economical way to building GenAI applications.\n",
    "\n",
    "Given a user query, RAG extracts relevant facts based on this query, and uses the LLM to summarize those facts into a cohesive response to the user query, which is then displayed to the user. You can see an example RAG question-answering application in [asknews](https://asknews.demo.vectara.com).\n",
    "\n",
    "For LLM applications built with Vectara, it can be useful to evaluate the response from the LLM to the extracted facts and provide the user with an indicator of the factual consistency of the response with the facts.\n",
    "\n",
    "For example, one can compute the HHEM score for each fact with the response and then average those score to provide an overall score. Let's look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c86f6315-dac5-4666-9d34-586469621137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We query the vectara.com website content\n",
    "# Customer-ID, corpus-ID and API key taken from create-ui\n",
    "\n",
    "import os\n",
    "os.environ['VECTARA_API_KEY'] = 'zqt_UXrBcnI2UXINZkrv4g1tQPhzj02vfdtqYJIDiA'\n",
    "os.environ['VECTARA_CORPUS_ID'] = '1'\n",
    "os.environ['VECTARA_CUSTOMER_ID']='1366999410'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1a8cd08-89cd-4198-8f16-6e4b933904a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def vectara_query(\n",
    "    query: str,\n",
    "    config: dict,\n",
    ") -> None:\n",
    "    \"\"\"Query Vectara and return the results.\n",
    "    Args:\n",
    "        query: query string\n",
    "    \"\"\"\n",
    "    corpus_key = [\n",
    "        {\n",
    "            \"customerId\": config[\"customer_id\"],\n",
    "            \"corpusId\": config[\"corpus_id\"],\n",
    "            \"lexicalInterpolationConfig\": {\"lambda\": config[\"lambda_val\"]},\n",
    "        }\n",
    "    ]\n",
    "    data = {\n",
    "        \"query\": [\n",
    "            {\n",
    "                \"query\": query,\n",
    "                \"start\": 0,\n",
    "                \"numResults\": config[\"top_k\"],\n",
    "                \"contextConfig\": {\n",
    "                    \"sentencesBefore\": 2,\n",
    "                    \"sentencesAfter\": 2,\n",
    "                },\n",
    "                \"corpusKey\": corpus_key,\n",
    "                \"summary\": [\n",
    "                    {\n",
    "                        \"responseLang\": \"eng\",\n",
    "                        \"maxSummarizedResults\": 5,\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        \"x-api-key\": config[\"api_key\"],\n",
    "        \"customer-id\": config[\"customer_id\"],\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    response = requests.post(\n",
    "        headers=headers,\n",
    "        url=\"https://api.vectara.io/v1/query\",\n",
    "        data=json.dumps(data),\n",
    "    )\n",
    "    if response.status_code != 200:\n",
    "        print(\n",
    "            \"Query failed %s\",\n",
    "            f\"(code {response.status_code}, reason {response.reason}, details \"\n",
    "            f\"{response.text})\",\n",
    "        )\n",
    "        return []\n",
    "\n",
    "    result = response.json()\n",
    "    responses = result[\"responseSet\"][0][\"response\"]\n",
    "    documents = result[\"responseSet\"][0][\"document\"]\n",
    "    summary = result[\"responseSet\"][0][\"summary\"][0][\"text\"]\n",
    "\n",
    "    res = [[r['text'], r['score']] for r in responses]\n",
    "    return res, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2edbe08e-8bfa-4a76-8fe9-74791dcf2897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectara is an end-to-end platform that offers powerful generative AI capabilities for product builders [1]. It enhances traditional searches by understanding the context and meaning of data [1]. Vectara enables the construction of semantic search applications powered by LLMs, which are deep neural nets designed to understand human language [2]. It provides a secure environment that respects data sovereignty and protects user data [3]. By grounding search results in uploaded data and reducing hallucinations, Vectara ensures accurate and trustworthy responses [4]. Additionally, Vectara supports cross-language search, eliminating language barriers for users worldwide [4]. However, the ability to retrieve text is limited, and Vectara mainly retains metadata for document retrieval [5].\n"
     ]
    }
   ],
   "source": [
    "api_key = os.environ.get(\"VECTARA_API_KEY\", \"\")\n",
    "customer_id = os.environ.get(\"VECTARA_CUSTOMER_ID\", \"\")\n",
    "corpus_id = os.environ.get(\"VECTARA_CORPUS_ID\", \"\")\n",
    "\n",
    "config = {\n",
    "    \"api_key\": str(api_key),\n",
    "    \"customer_id\": str(customer_id),\n",
    "    \"corpus_id\": str(corpus_id),\n",
    "    \"lambda_val\": 0.025,\n",
    "    \"top_k\": 10,\n",
    "}\n",
    "\n",
    "query = \"What does Vectara do?\"\n",
    "results, summary = vectara_query(query, config)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e428485-52bd-4480-957e-df14fcf489bf",
   "metadata": {},
   "source": [
    "Now let's compare each of the facts extracted (results) with the summary using HHEM. We only use the top 5 results, since that is what we asked the summarization to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59de07b4-b683-49e0-a909-9f79f87b0466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.width', 100)\n",
    "pd.set_option('display.max_colwidth', None)  # Use None to display full content without truncation\n",
    "\n",
    "texts = [r[0] for r in results[:5]]\n",
    "scores = [model.predict([text, summary]) for text in texts]\n",
    "df = pd.DataFrame({'fact': texts, 'HHEM score': scores})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "055fb391-db29-4e0b-b345-ccdad2a99976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Vectara is an end-to-end platform that offers powerful generative AI capabilities for product builders [1]. It enhances traditional searches by understanding the context and meaning of data [1]. Vectara enables the construction of semantic search applications powered by LLMs, which are deep neural nets designed to understand human language [2]. It provides a secure environment that respects data sovereignty and protects user data [3]. By grounding search results in uploaded data and reducing hallucinations, Vectara ensures accurate and trustworthy responses [4]. Additionally, Vectara supports cross-language search, eliminating language barriers for users worldwide [4]. However, the ability to retrieve text is limited, and Vectara mainly retains metadata for document retrieval [5].'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bccb1b0-1fd1-468e-9ae7-c8ae6d68d77b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fact</th>\n",
       "      <th>HHEM score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the Vectara Platform? | Vectara Docs Welcome to the documentation homepage for Vectara , an end-to-end platform for product builders to embed powerful generative AI capabilities into applications with extraordinary results. Vectara offers significant improvements over traditional searches by understanding the context and meaning of your data.</td>\n",
       "      <td>0.894155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Semantic Search Fundamentals\\nVectara lets you build a semantic, LLM-powered search application. Semantic\\nsearch is not just about finding data, but about understanding data and\\nhelping you answer questions about your data. This topic outlines what Vectara\\ncan do for this use case as well as why and how to employ these features for\\nthe best overall end-user experience. LLMs are deep neural nets\\nthat are built with the task of specifically understanding human language. These\\nmodels can be a great asset to many different use cases, including search and\\nlanguage generation.</td>\n",
       "      <td>0.494370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This versatile Vectara GenAI platform caters to a wide range of use cases to drive better outcomes and unlock new possibilities in search applications. Vectara provides an easy entry point to generative AI capabilities while protecting company IP and customer data. The data is secure. Vectara does not train on user data and respects data sovereignty and provides you with peace of mind. You might be wondering what kind of data to select for ingestion. Our Vectara Quick Start Tutorial provides an example that gets you set up and searching for answers quickly!</td>\n",
       "      <td>0.577135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>These hallucinations lead to inaccurate and misleading responses Vectara addresses this problem through Grounded Generation, meaning it grounds the search results in the uploaded data. By focusing on facts and reducing hallucinations, Vectara enhances trust in AI-powered decision making. Use Vectara to search across multiple languages, eliminating language barriers and enabling users to find what they need, regardless of the language they use. This cross-language approach provides a seamless search experience for users around the world. The best answer may be written in German but a user asked the question in Spanish.</td>\n",
       "      <td>0.425130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>At that point, the text is no longer recoverable. It also won't be returned in any Vectara APIs. Note that Vectara does retain any metadata that were supplied alongside the text, including document IDs. This retention allows you to retrieve the document from a separate system of record based on the ID to show the metadata, and it also allows Vectara to perform any metadata-based filtering on the document. Currently, the reranking capability relies on the text being stored.</td>\n",
       "      <td>0.392353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                fact  \\\n",
       "0                                                                                                                                                                                                                                                                                   What is the Vectara Platform? | Vectara Docs Welcome to the documentation homepage for Vectara , an end-to-end platform for product builders to embed powerful generative AI capabilities into applications with extraordinary results. Vectara offers significant improvements over traditional searches by understanding the context and meaning of your data.   \n",
       "1                                           Semantic Search Fundamentals\\nVectara lets you build a semantic, LLM-powered search application. Semantic\\nsearch is not just about finding data, but about understanding data and\\nhelping you answer questions about your data. This topic outlines what Vectara\\ncan do for this use case as well as why and how to employ these features for\\nthe best overall end-user experience. LLMs are deep neural nets\\nthat are built with the task of specifically understanding human language. These\\nmodels can be a great asset to many different use cases, including search and\\nlanguage generation.   \n",
       "2                                                                This versatile Vectara GenAI platform caters to a wide range of use cases to drive better outcomes and unlock new possibilities in search applications. Vectara provides an easy entry point to generative AI capabilities while protecting company IP and customer data. The data is secure. Vectara does not train on user data and respects data sovereignty and provides you with peace of mind. You might be wondering what kind of data to select for ingestion. Our Vectara Quick Start Tutorial provides an example that gets you set up and searching for answers quickly!   \n",
       "3  These hallucinations lead to inaccurate and misleading responses Vectara addresses this problem through Grounded Generation, meaning it grounds the search results in the uploaded data. By focusing on facts and reducing hallucinations, Vectara enhances trust in AI-powered decision making. Use Vectara to search across multiple languages, eliminating language barriers and enabling users to find what they need, regardless of the language they use. This cross-language approach provides a seamless search experience for users around the world. The best answer may be written in German but a user asked the question in Spanish.   \n",
       "4                                                                                                                                                      At that point, the text is no longer recoverable. It also won't be returned in any Vectara APIs. Note that Vectara does retain any metadata that were supplied alongside the text, including document IDs. This retention allows you to retrieve the document from a separate system of record based on the ID to show the metadata, and it also allows Vectara to perform any metadata-based filtering on the document. Currently, the reranking capability relies on the text being stored.   \n",
       "\n",
       "   HHEM score  \n",
       "0    0.894155  \n",
       "1    0.494370  \n",
       "2    0.577135  \n",
       "3    0.425130  \n",
       "4    0.392353  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c484025d-aa3a-44d2-9a74-aec5c2ab6c66",
   "metadata": {},
   "source": [
    "In an application using Vectara and HHEM we may for example display the average HHEM score, or maximum HHEM score, next to the summary to provide the user an indicator for the overall correctness of the summary given the individual facts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6612bdd-ec7e-44c7-8ade-c41803257a11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
